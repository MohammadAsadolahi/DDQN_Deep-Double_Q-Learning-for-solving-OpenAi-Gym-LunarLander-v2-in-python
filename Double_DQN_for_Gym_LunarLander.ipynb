{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8x-y8DACYp0A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential, load_model\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY4U5CiFZm_T",
        "outputId": "d64e1d3f-45c4-4e7e-973a-ce84ed015d4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.1.1.post1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1.post1\n",
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2373078 sha256=71e135bd190080d97bc6d411ad57ef93c58ee2ac6aaa729cfd207438884c2547\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "\u001b[33mWARNING: gym 0.25.2 does not provide the extra 'box_2d'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[Box_2D]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[Box_2D]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[Box_2D]) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install swig\n",
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IlBJ2Ho4UAuy"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/Drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CbiEYt9YyTr",
        "outputId": "d49fc7fc-03b5-4b59-8347-96d5ab777c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('LunarLander-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dLT6a5HGJZEe"
      },
      "outputs": [],
      "source": [
        "class replayBuffer:\n",
        "  def __init__(self,maxSize,stateDim):\n",
        "    self.state=np.zeros((maxSize,stateDim))\n",
        "    self.action=np.zeros(maxSize,dtype= np.int8)\n",
        "    self.reward=np.zeros(maxSize)\n",
        "    self.done=np.zeros(maxSize,)\n",
        "    self.nextState=np.zeros((maxSize,stateDim))\n",
        "    self.maxSize=maxSize\n",
        "    self.curser=0\n",
        "    self.size=0\n",
        "\n",
        "  def save(self,state,action,reward,nextState,done):\n",
        "    self.state[self.curser]=state\n",
        "    self.action[self.curser]=action\n",
        "    self.reward[self.curser]=reward\n",
        "    self.nextState[self.curser]=nextState\n",
        "    self.done[self.curser]=done\n",
        "    self.curser=(self.curser+1)%self.maxSize\n",
        "    if self.size<self.maxSize:\n",
        "      self.size+=1\n",
        "\n",
        "  def sample(self,batchSize):\n",
        "    batchSize=min(self.size,batchSize-1)\n",
        "    indexes=np.random.choice([i for i in range(self.size-1)],batchSize)\n",
        "    return self.state[indexes],self.action[indexes],self.reward[indexes],self.nextState[indexes],self.done[indexes]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y37LHqC2RQEs"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "  def __init__(self,stateShape,actionShape,exploreRate,exploreRateDecay,minimumExploreRate,gamma,copyNetsCycle):\n",
        "      self.gamma=gamma\n",
        "      self.exploreRate=exploreRate\n",
        "      self.exploreRateDecay=exploreRateDecay\n",
        "      self.minimumExploreRate=minimumExploreRate\n",
        "      self.actionShape=actionShape\n",
        "      self.memory=replayBuffer(1000000,stateShape)\n",
        "      self.model=self.buildModel(stateShape,actionShape)\n",
        "      self.model.compile(optimizer='Adam',loss='mse')\n",
        "      self.tModel=self.buildModel(stateShape,actionShape)\n",
        "      self.tModel.compile(optimizer='Adam',loss='mse')\n",
        "      self.learnThreshold=0\n",
        "      self.copyNetsCycle=copyNetsCycle\n",
        "\n",
        "  def buildModel(self,input,output):\n",
        "    inputLayer=keras.Input(shape=(input,))\n",
        "    layer=Dense(256,activation='relu')(inputLayer)\n",
        "    layer=Dense(256,activation='relu')(layer)\n",
        "    outputLayer=Dense(output)(layer)\n",
        "    model=keras.Model(inputs=inputLayer,outputs=outputLayer)\n",
        "    model.compile(optimizer='Adam',loss='mse')\n",
        "    return model\n",
        "\n",
        "  def getAction(self,state):\n",
        "    q=self.model.predict(np.expand_dims(state,axis=0),verbose=0)[0]\n",
        "    if np.random.random()<=self.exploreRate:\n",
        "      return np.random.choice([i for i in range(env.action_space.n)])\n",
        "    else:\n",
        "      return np.argmax(q)\n",
        "\n",
        "  def exploreDecay(self):\n",
        "      self.exploreRate=max(self.exploreRate*self.exploreRateDecay,self.minimumExploreRate)\n",
        "\n",
        "  def saveModel(self,modelName=\"DoubleDQN_LunarLanderV2.h\"):\n",
        "      self.model.save_weights(f\"{modelName}\")\n",
        "\n",
        "  def loadModel(self,modelName=\"DoubleDQN_LunarLanderV2.h\"):\n",
        "      self.model.load_weights(f\"{modelName}\")\n",
        "      self.tModel.set_weights(self.model.get_weights())\n",
        "\n",
        "  def learn(self,batchSize=64):\n",
        "    if self.memory.size>batchSize:\n",
        "      states,actions,rewards,nextStates,done=self.memory.sample(batchSize)\n",
        "      qState=self.model.predict(states,verbose=0)\n",
        "      qNextState=self.model.predict(nextStates,verbose=0)\n",
        "      qNextStateTarget=self.tModel.predict(nextStates,verbose=0)\n",
        "      maxActions=np.argmax(qNextState,axis=1)\n",
        "      batchIndex = np.arange(batchSize-1, dtype=np.int32)\n",
        "      qState[batchIndex,actions]=(rewards+(self.gamma*qNextStateTarget[batchIndex,maxActions.astype(int)]*(1-done)))\n",
        "      _=self.model.fit(x=states,y=qState,verbose=0)\n",
        "      self.learnThreshold+=1\n",
        "      self.exploreDecay()\n",
        "      if(self.learnThreshold%self.copyNetsCycle)==0:\n",
        "        self.tModel.set_weights(self.model.get_weights())\n",
        "        self.saveModel()\n",
        "        self.learnThreshold=0\n",
        "agent=Agent(stateShape=env.observation_space.shape[0],actionShape=env.action_space.n\\\n",
        "            ,exploreRate=0.01,exploreRateDecay=0.99,minimumExploreRate=0.01,gamma=0.99,copyNetsCycle=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M3fwkBPoSXHb"
      },
      "outputs": [],
      "source": [
        "agent=Agent(stateShape=env.observation_space.shape[0],actionShape=env.action_space.n\\\n",
        "            ,exploreRate=1.0,exploreRateDecay=0.9995,minimumExploreRate=0.01,gamma=0.99,copyNetsCycle=100)\n",
        "# agent.loadModel(\"DoubleDQN_LunarLanderV2.h\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld9e0nIucmo_",
        "outputId": "4023a74d-3766-4500-f9f5-a59a32f76fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 1   reward: -239.5577583572084  avg so far:-239.5577583572084 exploreRate:0.973841589962245\n",
            "episode: 2   reward: -72.22139745003793  avg so far:-155.88957790362315 exploreRate:0.9441083142062507\n",
            "episode: 3   reward: -135.80708745778986  avg so far:-149.19541442167872 exploreRate:0.8851213628869913\n",
            "episode: 4   reward: -294.5727647475836  avg so far:-185.53975200315494 exploreRate:0.8360684720191865\n",
            "episode: 5   reward: -50.664738466759346  avg so far:-158.56474929587583 exploreRate:0.8085173483727683\n",
            "episode: 6   reward: -224.96765046337572  avg so far:-169.63189949045912 exploreRate:0.7576229851573638\n",
            "episode: 7   reward: -271.96292871545336  avg so far:-184.2506179511726 exploreRate:0.7056844090876688\n",
            "episode: 8   reward: -346.47921575265923  avg so far:-204.5291926763584 exploreRate:0.6629187904796937\n",
            "episode: 9   reward: -15.023281044583442  avg so far:-183.47298027282784 exploreRate:0.6211895316420278\n",
            "episode: 10   reward: -131.53858161138277  avg so far:-178.27954040668334 exploreRate:0.582669560440043\n",
            "episode: 11   reward: -121.4198899802169  avg so far:-173.11048127700457 exploreRate:0.5052657846618471\n",
            "episode: 12   reward: -149.072593148328  avg so far:-171.10732393294816 exploreRate:0.45854587398317365\n",
            "episode: 13   reward: -227.84117958216325  avg so far:-175.47146667519547 exploreRate:0.42391828281840016\n"
          ]
        }
      ],
      "source": [
        "averageRewards=[]\n",
        "totalRewards=[]\n",
        "for i in range(1,100):\n",
        "  done=False\n",
        "  state=env.reset()\n",
        "  rewards=0\n",
        "  while not done:\n",
        "    action=agent.getAction(state)\n",
        "    nextState,reward,done,info=env.step(action)\n",
        "    agent.memory.save(state,action,reward,nextState,int(done))\n",
        "    rewards+=reward\n",
        "    state=nextState\n",
        "    agent.learn(batchSize=64)\n",
        "  totalRewards.append(rewards)\n",
        "  averageRewards.append(sum(totalRewards)/len(totalRewards))\n",
        "  print(f\"episode: {i}   reward: {rewards}  avg so far:{averageRewards[-1]} exploreRate:{agent.exploreRate}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFBZ-TCb8m48"
      },
      "outputs": [],
      "source": [
        "plt.title(f'Total Rewards')\n",
        "plt.yscale('symlog')\n",
        "plt.plot(totalRewards)\n",
        "plt.savefig(\"Total Rewards\",dpi=200)\n",
        "plt.clf()\n",
        "plt.title(f'Average Rewards')\n",
        "plt.yscale('symlog')\n",
        "plt.plot(averageRewards)\n",
        "plt.savefig(\"Average Rewards\",dpi=200)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
