{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8x-y8DACYp0A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential, load_model\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY4U5CiFZm_T",
        "outputId": "754304f0-6626-4c3f-a91d-bd566fc9f9c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30 kB 30.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 112 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 448 kB 8.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlBJ2Ho4UAuy",
        "outputId": "dde5ec80-681c-4a1d-f963-0e54ebdc879f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/Drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6CbiEYt9YyTr"
      },
      "outputs": [],
      "source": [
        "env = gym.make('LunarLander-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dLT6a5HGJZEe"
      },
      "outputs": [],
      "source": [
        "class replayBuffer:\n",
        "  def __init__(self,maxSize,stateDim):\n",
        "    self.state=np.zeros((maxSize,stateDim))\n",
        "    self.action=np.zeros(maxSize,dtype= np.int8)\n",
        "    self.reward=np.zeros(maxSize)\n",
        "    self.done=np.zeros(maxSize,)\n",
        "    self.nextState=np.zeros((maxSize,stateDim))\n",
        "    self.maxSize=maxSize\n",
        "    self.curser=0\n",
        "    self.size=0\n",
        "\n",
        "  def save(self,state,action,reward,nextState,done):\n",
        "    self.state[self.curser]=state\n",
        "    self.action[self.curser]=action\n",
        "    self.reward[self.curser]=reward\n",
        "    self.nextState[self.curser]=nextState\n",
        "    self.done[self.curser]=done\n",
        "    self.curser=(self.curser+1)%self.maxSize\n",
        "    if self.size<self.maxSize:\n",
        "      self.size+=1 \n",
        "      \n",
        "  def sample(self,batchSize):\n",
        "    batchSize=min(self.size,batchSize-1)\n",
        "    indexes=np.random.choice([i for i in range(self.size-1)],batchSize)\n",
        "    return self.state[indexes],self.action[indexes],self.reward[indexes],self.nextState[indexes],self.done[indexes]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Y37LHqC2RQEs"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "  def __init__(self,stateShape,actionShape,exploreRate,exploreRateDecay,minimumExploreRate,gamma,copyNetsCycle):\n",
        "      self.gamma=gamma\n",
        "      self.exploreRate=exploreRate\n",
        "      self.exploreRateDecay=exploreRateDecay\n",
        "      self.minimumExploreRate=minimumExploreRate\n",
        "      self.actionShape=actionShape\n",
        "      self.memory=replayBuffer(1000000,stateShape)\n",
        "      self.model=self.buildModel(stateShape,actionShape)\n",
        "      self.model.compile(optimizer='Adam',loss='mse')\n",
        "      self.tModel=self.buildModel(stateShape,actionShape)\n",
        "      self.tModel.compile(optimizer='Adam',loss='mse')\n",
        "      self.learnThreshold=0\n",
        "      self.copyNetsCycle=copyNetsCycle\n",
        "\n",
        "  def buildModel(self,input,output):\n",
        "    inputLayer=keras.Input(shape=(input,))\n",
        "    layer=Dense(256,activation='relu')(inputLayer)\n",
        "    layer=Dense(256,activation='relu')(layer)\n",
        "    outputLayer=Dense(output)(layer)\n",
        "    model=keras.Model(inputs=inputLayer,outputs=outputLayer)\n",
        "    model.compile(optimizer='Adam',loss='mse')\n",
        "    return model\n",
        "    \n",
        "  def getAction(self,state):\n",
        "    q=self.model.predict(np.expand_dims(state,axis=0))[0]\n",
        "    if np.random.random()<=self.exploreRate:\n",
        "      return np.random.choice([i for i in range(env.action_space.n)])\n",
        "    else:\n",
        "      return np.argmax(q)\n",
        "\n",
        "  def exploreDecay(self):\n",
        "      self.exploreRate=max(self.exploreRate*self.exploreRateDecay,self.minimumExploreRate)\n",
        "\n",
        "  def saveModel(self,modelName=\"DoubleDQN_LunarLanderV2.h\"):\n",
        "      self.model.save_weights(f\"{modelName}\")\n",
        "\n",
        "  def loadModel(self,modelName=\"DoubleDQN_LunarLanderV2.h\"):\n",
        "      self.model.load_weights(f\"{modelName}\")\n",
        "      self.tModel.set_weights(self.model.get_weights())\n",
        "      \n",
        "  def learn(self,batchSize=64):\n",
        "    if self.memory.size>batchSize:\n",
        "      states,actions,rewards,nextStates,done=self.memory.sample(batchSize)\n",
        "      qState=self.model.predict(states)\n",
        "      qNextState=self.model.predict(nextStates)\n",
        "      qNextStateTarget=self.tModel.predict(nextStates)\n",
        "      maxActions=np.argmax(qNextState,axis=1)\n",
        "      batchIndex = np.arange(batchSize-1, dtype=np.int32)\n",
        "      qState[batchIndex,actions]=(rewards+(self.gamma*qNextStateTarget[batchIndex,maxActions.astype(int)]*(1-done)))\n",
        "      _=self.model.fit(x=states,y=qState,verbose=0)\n",
        "      self.learnThreshold+=1\n",
        "      self.exploreDecay()\n",
        "      if(self.learnThreshold%self.copyNetsCycle)==0:\n",
        "        self.tModel.set_weights(self.model.get_weights())\n",
        "        self.saveModel()\n",
        "        self.learnThreshold=0\n",
        "agent=Agent(stateShape=env.observation_space.shape[0],actionShape=env.action_space.n\\\n",
        "            ,exploreRate=0.01,exploreRateDecay=0.99,minimumExploreRate=0.01,gamma=0.99,copyNetsCycle=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "M3fwkBPoSXHb"
      },
      "outputs": [],
      "source": [
        "agent=Agent(stateShape=env.observation_space.shape[0],actionShape=env.action_space.n\\\n",
        "            ,exploreRate=1.0,exploreRateDecay=0.9995,minimumExploreRate=0.01,gamma=0.99,copyNetsCycle=100)\n",
        "# agent.loadModel(\"DoubleDQN_LunarLanderV2.h\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "averageRewards=[]\n",
        "totalRewards=[]\n",
        "for i in range(1,200):\n",
        "  done=False\n",
        "  state=env.reset()\n",
        "  rewards=0\n",
        "  while not done:\n",
        "    action=agent.getAction(state)\n",
        "    nextState,reward,done,info=env.step(action)\n",
        "    agent.memory.save(state,action,reward,nextState,int(done))\n",
        "    rewards+=reward\n",
        "    state=nextState\n",
        "    agent.learn(batchSize=64)\n",
        "  totalRewards.append(rewards)\n",
        "  averageRewards.append(sum(totalRewards)/len(totalRewards))\n",
        "  print(f\"episode: {i}   reward: {rewards}  avg so far:{averageRewards[-1]} exploreRate:{agent.exploreRate}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ld9e0nIucmo_",
        "outputId": "8af4ced0-b469-4fb0-a380-e6441db208e9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 1   reward: -236.54794123174165  avg so far:-236.54794123174165 exploreRate:0.9714094193743169\n",
            "episode: 2   reward: -110.65996455333253  avg so far:-173.60395289253708 exploreRate:0.9333105749632893\n",
            "episode: 3   reward: -136.71824405049307  avg so far:-161.30871661185574 exploreRate:0.900751257125721\n",
            "episode: 4   reward: -177.99176052455215  avg so far:-165.47947759002983 exploreRate:0.8611062428400729\n",
            "episode: 5   reward: -246.38684905546668  avg so far:-181.6609518831172 exploreRate:0.81419825010847\n",
            "episode: 6   reward: -99.08303920576799  avg so far:-167.89796643689235 exploreRate:0.7538433821357534\n",
            "episode: 7   reward: -46.46007925584878  avg so far:-150.5496968396004 exploreRate:0.7235534186586253\n",
            "episode: 8   reward: -248.9914726607256  avg so far:-162.85491881724104 exploreRate:0.6820886413796821\n",
            "episode: 9   reward: -49.425562881514594  avg so far:-150.25165704660478 exploreRate:0.6420360739416774\n",
            "episode: 10   reward: -57.17477529149626  avg so far:-140.94396887109392 exploreRate:0.5932552444421718\n",
            "episode: 11   reward: -123.86764217094449  avg so far:-139.3915755347167 exploreRate:0.5389381029368515\n",
            "episode: 12   reward: -168.8881279022222  avg so far:-141.84962156534218 exploreRate:0.5067842382020455\n",
            "episode: 13   reward: -115.66698699710716  avg so far:-139.83557275240102 exploreRate:0.4661765688049959\n",
            "episode: 14   reward: -263.7877247069902  avg so far:-148.68929789201454 exploreRate:0.4066815021114777\n",
            "episode: 15   reward: -96.60189602797712  avg so far:-145.21680443441204 exploreRate:0.37222860556614584\n",
            "episode: 16   reward: -123.79784875472274  avg so far:-143.87811970443147 exploreRate:0.31940820807296777\n",
            "episode: 17   reward: -101.20586439909327  avg so far:-141.36798703941156 exploreRate:0.2892944928300246\n",
            "episode: 18   reward: -113.25461953180516  avg so far:-139.80613328898897 exploreRate:0.1974231991726412\n",
            "episode: 19   reward: 73.4058620873152  avg so far:-128.58444932181507 exploreRate:0.1806077199774845\n",
            "episode: 20   reward: -216.2449655857072  avg so far:-132.9674751350097 exploreRate:0.11653805241342109\n",
            "episode: 21   reward: -56.97100027541983  avg so far:-129.34859537979113 exploreRate:0.07067506393051091\n",
            "episode: 22   reward: -93.14097381446359  avg so far:-127.70279439954896 exploreRate:0.042861233375190315\n",
            "episode: 23   reward: -9.824202037099415  avg so far:-122.57763820987724 exploreRate:0.025993401693258956\n",
            "episode: 24   reward: -37.415442824059795  avg so far:-119.02921340213486 exploreRate:0.015763823818896745\n",
            "episode: 25   reward: -364.4812130346577  avg so far:-128.84729338743577 exploreRate:0.014515209797678797\n",
            "episode: 26   reward: -74.373086291723  avg so far:-126.75213157606221 exploreRate:0.01\n",
            "episode: 27   reward: -260.90348687304765  avg so far:-131.72070029076536 exploreRate:0.01\n",
            "episode: 28   reward: -337.76058366615524  avg so far:-139.07926755417216 exploreRate:0.01\n",
            "episode: 29   reward: -187.64031521914427  avg so far:-140.7537864391712 exploreRate:0.01\n",
            "episode: 30   reward: -244.61166954408952  avg so far:-144.2157158760018 exploreRate:0.01\n",
            "episode: 31   reward: -284.88465760157413  avg so far:-148.7534236736009 exploreRate:0.01\n",
            "episode: 32   reward: -53.974829115202326  avg so far:-145.79159259365096 exploreRate:0.01\n",
            "episode: 33   reward: -283.5379334266445  avg so far:-149.9657241340447 exploreRate:0.01\n",
            "episode: 34   reward: -560.5321201063564  avg so far:-162.04120636852446 exploreRate:0.01\n",
            "episode: 35   reward: -348.76819389547785  avg so far:-167.37626315500881 exploreRate:0.01\n",
            "episode: 36   reward: -374.74194957310056  avg so far:-173.13642111106694 exploreRate:0.01\n",
            "episode: 37   reward: -124.30281783202555  avg so far:-171.81659399541718 exploreRate:0.01\n",
            "episode: 38   reward: -350.73814884008937  avg so far:-176.52505596501382 exploreRate:0.01\n",
            "episode: 39   reward: -554.6711764065103  avg so far:-186.2211103353086 exploreRate:0.01\n",
            "episode: 40   reward: -210.8227634683456  avg so far:-186.83615166363452 exploreRate:0.01\n",
            "episode: 41   reward: -84.5319616518384  avg so far:-184.34092751700533 exploreRate:0.01\n",
            "episode: 42   reward: -306.78897736393424  avg so far:-187.25635727526554 exploreRate:0.01\n",
            "episode: 43   reward: -18.832664213987783  avg so far:-183.33952720407305 exploreRate:0.01\n",
            "episode: 44   reward: -59.40790277379958  avg so far:-180.5228993761123 exploreRate:0.01\n",
            "episode: 45   reward: -133.75617471000376  avg so far:-179.48363882797656 exploreRate:0.01\n",
            "episode: 46   reward: -66.45698511675036  avg so far:-177.0265376603412 exploreRate:0.01\n",
            "episode: 47   reward: -257.72709257347174  avg so far:-178.74357074359932 exploreRate:0.01\n",
            "episode: 48   reward: -21.675599032641713  avg so far:-175.47132133295437 exploreRate:0.01\n",
            "episode: 49   reward: -97.21625997135972  avg so far:-173.87427926435038 exploreRate:0.01\n",
            "episode: 50   reward: -37.075867780621856  avg so far:-171.13831103467584 exploreRate:0.01\n",
            "episode: 51   reward: -68.54660151576692  avg so far:-169.12670888724625 exploreRate:0.01\n",
            "episode: 52   reward: -69.40235063154535  avg so far:-167.20893276694432 exploreRate:0.01\n",
            "episode: 53   reward: -90.69673744060037  avg so far:-165.7653064400322 exploreRate:0.01\n",
            "episode: 54   reward: -125.40869970955889  avg so far:-165.01796187094934 exploreRate:0.01\n",
            "episode: 55   reward: -109.32276803109644  avg so far:-164.00532198295198 exploreRate:0.01\n",
            "episode: 56   reward: -11.347062101692522  avg so far:-161.27928162792952 exploreRate:0.01\n",
            "episode: 57   reward: -91.31871749462024  avg so far:-160.05190330980128 exploreRate:0.01\n",
            "episode: 58   reward: -136.84046481589235  avg so far:-159.65170609438906 exploreRate:0.01\n",
            "episode: 59   reward: -79.88125112303459  avg so far:-158.2996644847051 exploreRate:0.01\n",
            "episode: 60   reward: -86.83696284715167  avg so far:-157.10861945741254 exploreRate:0.01\n",
            "episode: 61   reward: -81.43075149623115  avg so far:-155.86799867116366 exploreRate:0.01\n",
            "episode: 62   reward: -81.1135048666095  avg so far:-154.6622810291547 exploreRate:0.01\n",
            "episode: 63   reward: -89.32513837593831  avg so far:-153.6251835267227 exploreRate:0.01\n",
            "episode: 64   reward: -550.5642921724123  avg so far:-159.8273570993116 exploreRate:0.01\n",
            "episode: 65   reward: -179.08862632839129  avg so far:-160.1236843182205 exploreRate:0.01\n",
            "episode: 66   reward: -75.89000000531917  avg so far:-158.84741637408567 exploreRate:0.01\n",
            "episode: 67   reward: -83.66174633875247  avg so far:-157.7252421944538 exploreRate:0.01\n",
            "episode: 68   reward: -48.56843222670041  avg so far:-156.1199949890457 exploreRate:0.01\n",
            "episode: 69   reward: 181.0182543015196  avg so far:-151.23393340512445 exploreRate:0.01\n",
            "episode: 70   reward: -27.074225322663203  avg so far:-149.46022328966072 exploreRate:0.01\n",
            "episode: 71   reward: 211.71041428826055  avg so far:-144.3733129012393 exploreRate:0.01\n",
            "episode: 72   reward: -131.13198459541482  avg so far:-144.1894055636584 exploreRate:0.01\n",
            "episode: 73   reward: -192.39906921267666  avg so far:-144.84981191501484 exploreRate:0.01\n",
            "episode: 74   reward: -59.77394873333762  avg so far:-143.7001380882354 exploreRate:0.01\n",
            "episode: 75   reward: -71.88601556443369  avg so far:-142.74261645458475 exploreRate:0.01\n",
            "episode: 76   reward: -104.43300328849386  avg so far:-142.23854259713616 exploreRate:0.01\n",
            "episode: 77   reward: -58.48301563055168  avg so far:-141.15080848068703 exploreRate:0.01\n",
            "episode: 78   reward: 156.44488425198645  avg so far:-137.3354790866784 exploreRate:0.01\n",
            "episode: 79   reward: 178.14425020644404  avg so far:-133.34206479182873 exploreRate:0.01\n",
            "episode: 80   reward: -86.8294530854605  avg so far:-132.76065714549912 exploreRate:0.01\n",
            "episode: 81   reward: -207.23175659510733  avg so far:-133.68005343500047 exploreRate:0.01\n",
            "episode: 82   reward: -46.595841370493616  avg so far:-132.61805084884796 exploreRate:0.01\n",
            "episode: 83   reward: -69.87580758108057  avg so far:-131.86212020706762 exploreRate:0.01\n",
            "episode: 84   reward: -98.94005588238689  avg so far:-131.47019086986904 exploreRate:0.01\n",
            "episode: 85   reward: -43.02664326335581  avg so far:-130.42967854508655 exploreRate:0.01\n",
            "episode: 86   reward: -63.642896744192754  avg so far:-129.65308805902964 exploreRate:0.01\n",
            "episode: 87   reward: -166.70926077248205  avg so far:-130.0790210787245 exploreRate:0.01\n",
            "episode: 88   reward: -17.023095574571233  avg so far:-128.79429465254094 exploreRate:0.01\n",
            "episode: 89   reward: 184.46232220159146  avg so far:-125.274557384517 exploreRate:0.01\n",
            "episode: 90   reward: 60.41618578827024  avg so far:-123.21132690481936 exploreRate:0.01\n",
            "episode: 91   reward: -44.172454418641564  avg so far:-122.34276786650972 exploreRate:0.01\n",
            "episode: 92   reward: 121.33588969050692  avg so far:-119.69408680610736 exploreRate:0.01\n",
            "episode: 93   reward: -41.16576537970433  avg so far:-118.84969625313529 exploreRate:0.01\n",
            "episode: 94   reward: -75.08383176348003  avg so far:-118.38410195005386 exploreRate:0.01\n",
            "episode: 95   reward: -36.785330500052815  avg so far:-117.52516751373807 exploreRate:0.01\n",
            "episode: 96   reward: -64.1435062128911  avg so far:-116.96910854185425 exploreRate:0.01\n",
            "episode: 97   reward: -69.50245480755696  avg so far:-116.47976159613984 exploreRate:0.01\n",
            "episode: 98   reward: -50.896724721700366  avg so far:-115.81054693415577 exploreRate:0.01\n",
            "episode: 99   reward: -35.44976849124266  avg so far:-114.99882189937885 exploreRate:0.01\n",
            "episode: 100   reward: -469.78974200077147  avg so far:-118.54673110039279 exploreRate:0.01\n",
            "episode: 101   reward: -277.53374086341023  avg so far:-120.1208599099276 exploreRate:0.01\n",
            "episode: 102   reward: -63.95896472944835  avg so far:-119.57025309443272 exploreRate:0.01\n",
            "episode: 103   reward: -59.352245836599295  avg so far:-118.98561224726929 exploreRate:0.01\n",
            "episode: 104   reward: -238.72479464012244  avg so far:-120.13695053950825 exploreRate:0.01\n",
            "episode: 105   reward: -74.1429701114409  avg so far:-119.69891263066951 exploreRate:0.01\n",
            "episode: 106   reward: -34.23527576537198  avg so far:-118.8926519055252 exploreRate:0.01\n",
            "episode: 107   reward: 238.00394959945882  avg so far:-115.55716964846927 exploreRate:0.01\n",
            "episode: 108   reward: 212.82385474799008  avg so far:-112.51660460776131 exploreRate:0.01\n",
            "episode: 109   reward: 214.73058247979742  avg so far:-109.51433683631582 exploreRate:0.01\n",
            "episode: 110   reward: 227.98576594590355  avg so far:-106.4461540837502 exploreRate:0.01\n",
            "episode: 111   reward: 107.07605438532099  avg so far:-104.52253058402883 exploreRate:0.01\n",
            "episode: 112   reward: 261.754720663611  avg so far:-101.25219798360347 exploreRate:0.01\n",
            "episode: 113   reward: 253.64469499668422  avg so far:-98.11151751475137 exploreRate:0.01\n",
            "episode: 114   reward: 233.6373309161472  avg so far:-95.20143989693648 exploreRate:0.01\n",
            "episode: 115   reward: 218.53947596364202  avg so far:-92.47325801988796 exploreRate:0.01\n",
            "episode: 116   reward: 227.2090222470589  avg so far:-89.71737629344877 exploreRate:0.01\n",
            "episode: 117   reward: -116.5004824666922  avg so far:-89.94629173082691 exploreRate:0.01\n",
            "episode: 118   reward: 60.27353897832785  avg so far:-88.67324231803745 exploreRate:0.01\n",
            "episode: 119   reward: 204.80334189653277  avg so far:-86.20705253472174 exploreRate:0.01\n",
            "episode: 120   reward: 196.77728320138863  avg so far:-83.84884973692081 exploreRate:0.01\n",
            "episode: 121   reward: 225.9711310919011  avg so far:-81.28835402759171 exploreRate:0.01\n",
            "episode: 122   reward: 251.70854235878772  avg so far:-78.5588712703263 exploreRate:0.01\n",
            "episode: 123   reward: 248.5503414193705  avg so far:-75.89944677691415 exploreRate:0.01\n",
            "episode: 124   reward: 258.8573273307354  avg so far:-73.19979537282019 exploreRate:0.01\n",
            "episode: 125   reward: 216.79996594377974  avg so far:-70.8797972822874 exploreRate:0.01\n",
            "episode: 126   reward: 271.5402134140694  avg so far:-68.16217814977664 exploreRate:0.01\n",
            "episode: 127   reward: -176.69060991068528  avg so far:-69.01673273057119 exploreRate:0.01\n",
            "episode: 128   reward: 239.23461242783608  avg so far:-66.60851909652114 exploreRate:0.01\n",
            "episode: 129   reward: 230.762779672044  avg so far:-64.30331523009815 exploreRate:0.01\n",
            "episode: 130   reward: 258.4229303001295  avg so far:-61.8208056490964 exploreRate:0.01\n",
            "episode: 131   reward: 267.62700420262104  avg so far:-59.30593687160237 exploreRate:0.01\n",
            "episode: 132   reward: -42.96212003848461  avg so far:-59.18212007741209 exploreRate:0.01\n",
            "episode: 133   reward: 230.44389878570223  avg so far:-57.0044808378398 exploreRate:0.01\n",
            "episode: 134   reward: 0.3414523033322183  avg so far:-56.576526112905675 exploreRate:0.01\n",
            "episode: 135   reward: 268.30259770272954  avg so far:-54.170014084641714 exploreRate:0.01\n",
            "episode: 136   reward: 224.23712443042  avg so far:-52.12290277203096 exploreRate:0.01\n",
            "episode: 137   reward: 295.9216600344988  avg so far:-49.58243151066943 exploreRate:0.01\n",
            "episode: 138   reward: 271.60788699340674  avg so far:-47.254965434552936 exploreRate:0.01\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9d565e02dfd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrewards\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnextState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mtotalRewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0maverageRewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotalRewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotalRewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-9ad4657b3400>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batchSize)\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0mbatchIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0mqState\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mqNextStateTarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxActions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearnThreshold\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploreDecay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rFBZ-TCb8m48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "f56efa0d-0ca2-4fc3-b2c8-845ba1946089"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEICAYAAAC3Y/QeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU5d3/8fc3CQkJkAXCFgKEfV8EVBBxxw23utStrlVr1dZqW5favY+21p9t7VPrvtS6UhVFBZdHAQHZdxQCARJCFkgCIQkh69y/P2ZCYwwQMMmZzHxe15XLOefMnPOdI/OZe+5zn3PMOYeIiISXCK8LEBGR1qfwFxEJQwp/EZEwpPAXEQlDCn8RkTCk8BcRCUMKf5EwYmbOzAZ6XYd4T+EvnjGzuWa2x8xivK7l2zKz35pZtZmVmVmxmX1hZpO8rkvkYBT+4gkzSwOmAA64oAXWH9Xc62yCN5xzHYFkYA7wHw9qADx7/9KGKPzFK9cCi4EXgesAzCwm0GoeWfckM+tqZvvNrFtg+jwzW12vdT263nMzzexeM1sL7DOzKDO7z8y2mFmpmX1lZt+p9/xIM3vUzArNbJuZ3RHoFokKLE8ws+fMLM/Mcszsf8ws8nBvzDlXA7wC9DKzrodbl5llmdn4wOOrAzWMCEx/38zeCTw+zswWBd57npn9w8yi670fZ2a3m9lmYHNg3s8Dz801sxvr12lm5wb2SWmgpp819X+etH0Kf/HKtfgD8hXgLDPr7pyrBN4Grqz3vO8C85xzu8zsGOB54AdAF+ApYGaDbqMrgWlAYiCEt+D/hZEA/A542cx6Bp57M3AOMBYYB1zUoMYXgRpgIHAMcCZw0+HeWCCQrwWKgD1NWNc84JTA45OBrcBJ9abnBR7XAnfh/2UxCTgduK3B5i8CjgeGm9nZwM+AqcAg4IwGz30O+IFzrhMwEvjscO9NQohzTn/6a9U/4ESgGkgOTG8E7go8PgPYUu+5C4FrA4+fAP7QYF3pwMmBx5nAjYfZ9mrgwsDjz/CHH/W27YAooDtQCcTWW34lMOcg6/0tUAUU4w/pIuCUwLJDrgv4PjAz8HgD/i+F1wPTWcC4g2zzJ8CMetMOOK3e9PPAn+pNDw48Z2Bgejv+L9J4r/9N6K/1/9TyFy9cB3zsnCsMTL8amAf+vvI4Mzs+cFxgLDAjsKwv8NNAt0exmRUDvYGUeuvOrr8hM7u2XjdRMf4WbnJgcUqD59d/3BdoB+TVe+1TQLdDvK/pzrlE/GG/HhjfxHXNA6YEfpFEAtOByYH3n4D/CwszG2xm75tZvpmVAA/Vey+NvYeG7y+rwXMvAc4Fssxsng5QhxcdFJJWZWax+LtyIs0sPzA7Bkg0szHOuTVmNh1/y3gn8L5zrjTwvGzgQefcg4fYxIHL1JpZX+AZ/N0ji5xztWa2GrDAU/KA1Hqv7V3vcTb+1nqy83cfNZlzrtDMbgGWm9mrh1uXcy7DzMqBHwGfO+dKAvvmFmCBc84XeOoTwCrgSudcqZn9BLj0YO8/8P7qv6c+Dba7DLjQzNoBd+D/0qn/fAlhavlLa7sIf7fIcPyt+rHAMGA+/n5y8P8SuBy4OvC4zjPArYFfBWZmHcxsmpl1Osi2OuAPwwIAM7sBf8u/znTgTjPrZWaJwL11C5xzecDHwKNmFm9mEWY2wMxObsqbdM6lAx8B9zRxXfPwB3Bd//7cBtMAnYASoMzMhgI/PEwZ04HrzWy4mcUBv6lbYGbRgYPLCc656sB6fQdbkYQehb+0tuuAF5xz251z+XV/wD+Aq80syjm3BNiHv9tidt0LnXPL8R+k/Qf+A6kZwPUH25Bz7ivgUWAR/l8Ro/AfQ6jzDP5QXou/RT0L/0HZ2sDya4Fo4KvA9t4EetJ0jwC3BEYqHW5d8/CH++cHmQb/wdurgNJA7W8cauPOudnA3/Af28jgmwd0rwEyA11It+L/spUwYc7pZi4iAGZ2DvCkc66v17WItDS1/CVsmVlsYKx7lJn1wt8tMuNwrxMJBWr5S9gK9IPPA4YC+4EPgDudcyWeFibSChT+IiJhSN0+IiJhqE2M809OTnZpaWlelyEi0qasWLGi0DnXtbFlbSL809LSWL58uddliIi0KWbW8KzuA9TtIyIShhT+IiJhSOEvIhKGFP4iImFI4S8iEoYU/iIiYUjhLyIShhT+IiJByDnHH2dtYN2OvS2yfoW/iEgQWpVdzFOfb2VDXstcZ1DhLyIShN5euYP27SI4Z1SPFlm/wl9EJMhU1tTy3po8zhrRg07t27XINhT+IiJB5rMNu9i7v5pLxqW22DYU/iIiQeatlTvoHh/D5IHJLbYNhb+ISBApLKtkbnoBFx3Ti8gIa7HtKPxFRILIzNW51Phci3b5gMJfRCSovLVyB6N6JTC4e6cW3Y7CX0QkSGzML+HL3BIuGderxbel8BcRCRJvr8whKsI4f0xKi29L4S8iEgRqan3MWJXDqUO70aVjTItvT+EvIhIEFmQUUlBa2SpdPqDwFxEJCv9elEXnDtGcOrRbq2xP4S8i4rGN+SV8unEXN5yQRkxUZKtsU+EvIuKxJ+duoUN0JNdOSmu1bSr8RUQ8lL27nPfW5nHV8X1IiGuZi7g1RuEvIuKhZ+ZvJcLg+yf2b9XtKvxFRDxSUlHNmyt2cMGYXvRIaN+q21b4i4h4ZMbKHMqrarnuhL6tvm2Fv4iIB5xz/HtxFmN6JzI6NbHVt6/wFxHxwKItRWTsKuOaia3f6geFv4hIq6usqeXZBdtIjGvHeaN7elJDlCdbFREJQzW1Pv73swxeXpxF0b4q7jx9EO3btc5JXQ0p/EVEWsnbq3J47NPNnD60G9edkMaJLXibxsNR+IuItIJan+OJuVsYkRLPs9dNwKzlbtHYFOrzFxFpBR+sy2Nb4T5+dNpAz4MfFP4iIi3O53M8/lkGg7p15MzhPbwuB1C3j4hIi1mRtZtnPt9G1u5y0neW8rfLxxIR4X2rHxT+IiItotbnuHv6Gkr2VzMqNZGpwwZ6NqyzMQp/EZEW8MlXO8kqKufxq8YxLYhCv476/EVEWsBzC7aSmhTLWSO6e11KoxT+IiLNbHV2Mcsy93DD5H5ERQZnzAZnVSIibdiz87fSqX0Ulx/b2+tSDkrhLyLSjHbsKWf2+nyuOq4PHWOC97Cqwl9EpBm9uDATA647Ic3rUg5J4S8i0kxKKqp5fVk200b3JCUx1utyDknhLyLSTKYvy6assoabWvl+vEdD4S8i0gyqa328sDCT4/t1ZlRqgtflHJbCX0SkGby1Ygc5xfv5wcnB3+oHhb+IyLdWWVPL/36WwdjeiZw6pJvX5TSJwl9E5FuaviybnOL93D11cFBcrrkpFP4iIt9CRXUt/5iTwXFpnZkyyLs7cx0phb+IyLfwypLt7Cyp5O4z206rHxT+IiJHrbyqhifmZjB5YBcm9u/idTlHROEvInIEFm8tYta6PJxz/OuLLArLqrh76hCvyzpiwXvhCRGRIFNRXcvtr6ykaF8VU4d3Z1nmbk4Z0pXxfZO8Lu2IqeUvItJE/1mxg6J9VVx5XG/mpRdQXF7N3VMHe13WUVHLX0SkCWpqfTzz+VbG9k7koe+M4vsn9mNLwT5GpyZ6XdpRUfiLiDTBh1/ms313Ob84dxhmxsBunRjYrZPXZR01dfuIiByGc44n522hf3IHpg4PztsyHimFv4jIYby3No/1OSXcdupAIiPazlj+Q1H4i4gcQkV1LQ/P3siIlHguPqaX1+U0G4W/iMghPL9wGznF+3lg2jAiQqTVDwp/EZGDKiqr5J9ztnDGsO6cMKDtXLenKRT+IiIH8fT8rZRX1XDfOW3vDN7DUfiLiDSiqKySl77I4oIxKW16SOfBKPxFRBrx9PytVNbUcsdpg7wupUUo/EVEGvh6q7+j1+W0CIW/iEgDT8zdEtKtflD4i4h8TW7xfl5anMXF41JDttUPCn8Rka/5+6ebwcFPzgjdVj8o/EVEDthaUMZ/VuzgquP7kJoU53U5LUrhLyIS8Ognm4iJiuCO0wZ6XUqLU/iLiADrc/bywdo8bjqxH8kdY7wup8Up/EVEgEc+Sicxrh03ndTf61JahcJfRMLe4q1FzNtUwG2nDCC+fTuvy2kVCn8RCWvOOR7+cCM94ttz7aQ0r8tpNQp/EQlr76zOYdX2Yu6eOpj27SK9LqfVKPxFJGztq6zhT7M3Mjo1gUvHp3pdTqvSDdxFJGw9PieDnSWVPPG98SF1o5amUMtfRMJSxq4ynp2/jYvH9WJcnySvy2l1Cn8RCTs+n+O+t9YSGx3J/ecM87ocTyj8RSTsvLwki+VZe/j1ecPp2in0T+hqjMJfRMLKjj3lPDx7I1MGJXPxuF5el+MZhb+IhA3nHA/MWI8DHvrOKMzC6yBvfQp/EQkb76zOYd6mAu45awi9O4f2VTsPR+EvImGhsKyS3733FeP7JnFNGJ3JezAKfxEJeVsKyrjy6cWUV9by8CWjiAyzMf2N0UleIhJyamp9/OSN1RSXV9MrMZZZ6/JoFxXBCzccy8BunbwuLygo/EUk5Ly8OIv31+YxrGc863P3MrRnJ/52xTH0Soz1urSgofAXkZCyq6SCRz/exJRBybx043FhPaLnUNTnLyIh5aFZG6is8fH7C0cq+A9B4S8iIWPRliLeWZ3LrSf3p19yB6/LCWoKfxEJCVU1Pn717np6d47ltlND/wbs35b6/EUkJDy/cBsZu8p47roJYXVTlqOllr+ItHk5xft57P82M3V4d04f1t3rctoEhb+ItHl/eO8rHI7fnD/c61LaDIW/iLRpc9J38eGX+fzotEGkJoX39XqOhMJfRNqsiupafjvzSwZ07cDNU/p7XU6b4skBXzO7CJgGxAPPOec+9qIOEWnbnpq3layicl696Xiio9SWPRLNsrfM7Hkz22Vm6xvMP9vM0s0sw8zuq5vvnHvHOXczcCtweXPUICLhJbd4P0/My2Da6J6cMDDZ63LanOb6qnwROLv+DDOLBB4HzgGGA1eaWcOjMb8MPEdE5Ij8+cON+Bzcf85Qr0tpk5ol/J1znwO7G8w+Dshwzm11zlUBrwMXApjfw8Bs59zKxtZpZreY2XIzW15QUNAcZYpIiFi5fQ/vrM7llin9dZD3KLVkJ1kvILve9I7APIAfAWcAl5rZrY292Dn3tHNugnNuQteuXVuwTBFpS3w+x+/f+4punWL44SkDvC6nzWrSAV8z+z+gRyOLHnDOvXukG3XO/R34+5G+TkRk5ppcVmcX8/8uG0OHGF2k4Gg1ac855844inXnAL3rTacG5omIHJXyqhr+NHsjo1MTuPiYXod/gRxUS3b7LAMGmVk/M4sGrgBmtuD2RCTEPTVvK/klFfz6vOFE6FaM30pzDfV8DVgEDDGzHWb2fedcDXAH8BGwAZjunPuyObYnIuFn6bbdPDlvC+eN7smEtM5el9PmNUuHmXPuyoPMnwXMao5tiEj4Wpa5m+tfWEpqUiy/OX+E1+WEBJ0SJyJBLT2/lOufX0qPhPa8dvNEunaK8bqkkKDwF5GgVetz3PPWWtq3i+S1myfSLb691yWFDIW/iAStlxZlsia7mF+fP5zuCv5mpfAXkaCUU7yfRz5K5+TBXblgTIrX5YQchb+IBB3nHPe9tRbn4H8uGomZhnU2N4W/iASd15ZmM39zIb84dyi9O+vaPS1B4S8iQSV7dzkPfvAVkwd24erj+3pdTshS+ItI0PD5HPe8uRYz4+FLRuss3hak8BeRoPHykiwWbS3il9OG6VLNLUzhLyJHzTnH+py9bC0o+9bryizcxx9nbeTkwV25/Njeh3+BfCu6HqqIHBHnHBm7ypi3qYC3VuawIa+E5I7RfPazU4hv3+6o1lnX3RMVafzpklEa3dMKFP4icliFZZUszCjk802FLMgoYGdJJQAje8XzkzMG8dinm/nrJ5uO+ro7ryzdztLM3Txy6Wh6JsQ2Z+lyEAp/EWnUzpIK3liWzez1+WzIKwEgMa4dkwckM2VQMicOSj7QL19QWslLi7K4/NjeDO0Rf0Tbyd9bwcOzNzJ5YBcuHZ/a7O9DGqfwb0PKq2qIi9b/MmkZtT7Hoi1FLM3czarte/hiSxG1PsdxaZ35+VlDmDIomREpCUQ2MgLn52cNYda6PH45Yz3/uvG4b9xha/e+KuKiI2nfLvJr82tqffzq3fVU1/p48CJ197QmJUkbMX15Ng/MWMdnPz1FJ71Is6qoruWJuVuYvjybvL0VRBgM7t6Jm6b048pj+5CW3OGw60iMi+Y354/grumrOe9/F/DnS0ezt7yahVsKWZhRyKadZXTpEM3tpw7k3FE92ZhfwhdbipixKoeC0kruO2dok7Yjzcecc17XcFgTJkxwy5cv97oMzxSWVXL6o/PYu7+aP18ymu9qJIQ0E+ccd76+mplrcg+Msjl5cNejvjfuoi1F3PXGavJLKgCIiYrguH6dmdi/CwszCvliS9GB50ZFGKcO7cal41M5c3h3tfpbgJmtcM5NaGyZWv5twEOzNlBeVUOH6EhWZO1R+Euz+efcLcxck8s9Zw/htlMGfuv1TRrQhdl3TuG9tbkM7NaRcX2SDnT13HbKABZmFLExv4ThKfGM7JVw1KOD5NtT+Ae5xVuLeHtlDrefOoANeaWs2L7H65IkBGwvKufVpdt5ct4WLhqbwg9PHtBs607qEM21k9K+Md/MODFwoFi8p/APcn/9ZBM9E9pzx6mDeH7hNj7buIvi8ioS46K9Lk3aoKoaH/e/vY63Vu4gwuDcUT340yWj1eUShhT+QWx9zl6WbNvNA+cOIzY6kvF9kwBYuX0Ppw3t7nF10tbsq6zh1pdXMH9zIbec1J/rT0gjJVFj6sOVwj+IPbdgGx2iI7n8OH8f/5jURKIijBVZCn85MtW1Pq5/YSkrtxfz50tH890JOm4U7nRtnyC1s6SC99bkctmE3gcOisVGRzIiJZ7lmf5+/xVZu9kZGFUhcihPf76VZZl7ePSyMQp+ART+QevZ+VupdY4bJqd9bf64vkms2VHME3O3cMkTi7jj1ZXeFChtRsauUh77dDPnjurBRcf08rocCRIKfw9tKSjjLx+nU+v777kWpRXV/HT6Gp6Zv40LxqTQt8vXT3yZ0LczFdU+Hv5wI306x7Escw/LM3e3dunSRpRV1nDPm2uJi47kdxeM9LocCSLq8/fQq0u289yCbfTp0oFLx6dSWlHNhY8vJLNwHz8+bSA/On3QN15zXL/OJMS246KxKfz87KFMefgznpi7heeu7+zBO5Bgsr2onNnr88gt3s/gHp2oqvHx+JwMCsuqeOyKsXTtFON1iRJEFP4eWp1dDMBfPk7nvNE9+ePsjWQW7uOlG48/6Fjorp1iWPHLM4iK9P9ou2FyP/7yySY25pcc8QW1pO2rrvUxe30+Ly7cxsrt/n9PcdGRlFfVAnBcWmeeuXYox/RJ8rJMCUIKf49U1fhYn7OXMb0TWZNdzN3TVzNrXT43T+l32JNg6oIf4NpJfXlq3haemreVv14+tqXLFo9V1tTyxZYiPt9UQHp+KRvySthTXk2/5A784tyhnDOyJ6lJseQU76e4vJoRKfEawy+NUvh7ZGN+CZU1Pm6e0o//LN/BrHX5pHWJ4+6pQ45oPYlx0VwyPpU3lmXzh4tG0vEor8kiwSNjVynzNxeycnsxvZNiuWxCb2p9jhcWbuPd1bmUVdYQ2y6SIT06ccaw7pw9sgenDun2tfvdpibFkarGvhyCksIjdV0+Y3snMqhbJ/L3VvDgd0YSGx15mFd+03mjU3hpURafbtjJhWM1mqMtW7y1iKueWYzPQY/49nywtoJ/zt0CQHRUBOePTuG80T05YWAXYqKO/N+KSB2Fv0dWby+ma6cYeiXGYmZ8dNdJR72uCX2T6NYphlnr8hT+bVhVjY9fvrOelMRYXr9lIqlJcewsqWDGqhx8zvHdCb1J7qiDttI8FP4eWZVdzNjeic3SHxsRYZwzsgevL8umrLJGXT9t1DPzt5Kxq4znr59w4A5Z3ePbc2szXnRNpI7G+XuguLyKbYX7GNs7sdnWOW10CpU1Pj7dsLPZ1imtJ3t3Of/72WbOGtFdl+6QVqHw90Bdf/8xfZov/Ot3/Ujb4pzj/rfXEWl21DdAFzlSCv+jsHhrEbe/upKN+SUH5mUW7iNv7/4mvX5l1h7MYHRq84V/XdfPnI0F/Gd5Nm3hDm3i9/qybBZkFHLfucN0lU1pNeocPgKlFdX85t0veXtVDgALMwr5943HszanmN/O/JJRvRJ4+7bJB339vsoaHvt0M88t2Ma4PknN3jd/+6kD+SqvhJ+/uZYP1uXx2BXHkBCrOyUFs9zi/Tz4wQYm9u/M1cf18bocCSMK/yPwzPxtzFidwx2nDuSCsSnc8MIyLnniC6pqfSTFtWNVdjGFZZWNjsjYsaeca55byrbCfVw+oTf3nTO02evrFt+eN26ZxL8WZfLgBxu49821PPG9cTrJJ8hU1fhIzy/lrZU7eGvFDmp9jj9fMuZr4/RFWprC/wjM2biL8X2S+NlZ/hOxpt86iTtfW8UJA7pwxvDuXPCPhcxNL+DS8alfe932onKufGYxJRXVvHbzRCYN6NJiNUZEGDdM7kd1rY+HZm3kpUVZXHdCWottT5qmqKySmWtymbkml/U5e6mudbSLNKaN6sktJw2gT5c4r0uUMKPwb6LCskrW5ezlZ2cOPjCvV2Isb/7wBMB/0K57fAyfbdx5IPxran28szqXP3+4kapaH6/dPJGRvRJapd6bTuzP4q27efCDDYzvm9Rq2w1nS7YWsXTbbor3V9OpfRTH9+tCTLsI/vVFJh+szaPG5xiREs+NJ/ZjZEoCE/t30cXWxDMK/yaav7kAgJMHd2t0uZlx2tBuvL8mj6oaH9t37+OWf69ga8E+RqTE8+h3x7TqhdciIoxHLxvDmX/7nAdmrGPGbZOJiDCqanzs3V+t0GlmmYX7uPrZJdT4HHHRkVRU1+JzmwHoGBPFNZP6csWxfRjSo5PHlYr4KfybaG56AckdoxmRcvAAP3VIN15bms3c9F386cONlOyv5snvjeesEd096XdP6hDNL84dyl1vrOHNlTs4f3QKVz+7mE07y/jorpPopZElzeZv/7eJqEhjwb2n0SOhPXv3V7Ns2252l1dxzsgedGqvA+8SXBT+TVDrc3y+qeAbF89qaPLAZKKjIvjJG6upqK7llZtatn+/KS4a24t/L8rizx9uZNa6PFZlFxMdGcEvZ6zj+euP1cHgZpCeX8q7a3K55aT+9EhoD0BCbDvOGK6TtSR4aZx/E6zL2cue8mpOHtL1kM/rEBPFpP5dKK+q5WdnDfE8+MHfHfXbC0ZQtK+KuekF/P7Ckdx79lDmpBcwc02u1+WFhEc/TqdjdBS3nqTLMEjboZZ/IzbtLGVA145EBlr589ILMIMpgw4d/uAfaz8mNSGogmB0aiK/mjacdpHGNRP7UutzvLsml9+99xWnDe2mLomjtHd/NQ99sIGPv9rJ3VMHk9Qh2uuSRJpMLf8G8vbu5+y/fc4by7IPzPtiSyEjUuLp3IQP93H9OnP3mUOCbsz2jSf245pJaQBERhj3njWE3fuqWLpN9/89Gpt2ljL1L/N4c+UOfnjKAF18TdochX8DG/JK8DlYtLUI8N8mb+2OvUzoG1r3yB3XN4l2kcayzD1el9LmOOf45TvrqfE53rltMveePZToKH2UpG1Rt08Dm3aWAbAi098i3phXyv7qWsb1Da3bIrVvF8nIXgmsyArfln9NrY/5GYXMWJlDn85xB07eO5wP1+ezdNtuHvzOSEal6vwJaZsU/g1sDoR/7t4Kcov3s3K7v2U8PsTCH+DYtM68uDCTiupa2rcLn7tC7d5XxWtLt/PvRVnkl1QQFWHUOsfF43rRv2vHQ762orqWh2ZvYGiPTlw+oXcrVSzS/PRbtYGMXaUkd/T37S/P2sOKrD10j48hJTCEL5RM6JtEVa3/RvLhYENeCfe+uZZJf/yURz5KZ1D3jjz5vfF8fs+ptIuM4NkF2w75+j37qvjF2+vI3r2fX04bTlSkPj7SdqnlX4/P59i8q4xLxqXy1sodrMjczcrtexjfNykkx8PX/ZpZlrmHCWmhdUyjjs/nmJO+i+cWbOOLLUW0bxfBJeNTueGENAZ1/+/ZtpeM68WbK3Zw99TB37gw3559Vby2bDtPzt1CWWUNt548gBMHJbf2WxFpVgr/enL37qe8qpZhPeMZ2zuRT77aSe7eCq4P0QujdekYw4CuHVieuRsIrdEqZZU1vLMqh+cXbGNr4T56xLfn3rOHcuVxvUmM++aorZum9Of1Zdm89EUmd5/p7/tfk13MS4uyeG9tLlU1Pk4e3JUHpg1jcHddokHaPoV/PXX9/YO6d2RC3yS+2OIf8RNqB3vrm9C3Mx9+mY/P54JueOqRqK718clXO3l9WTYb80rYVVoJwOjUBB67YiznjupJu0N00wzo2pEzhnXniXlbmL0+HzP/wf+46Ei+OyGV703s26rXZhJpaWEV/s45fvz6as4d2YNzRvX8xvLNu0oBGNStI+VVtQBER0Uc8no+bd2EtCTeWJ5NRkFZm23RrsjazR2vriJvbwWpSbGcNLgraV3iOL5/FyYcQZfd7y8cwdOfx5JXXEFpZTVXH9+Xi8f10klwEpLCKvy3FJTx3ppcPtuwk+Ep8fTt0uFryzftLKNrpxgS46IZ1yeRCINRvRKIiQrdkTDH9/NfguKDtXkMntr2wn9O+i5++PIKeibE8uy1Ezh1aLcDZ2YfqZ4JsbqHroSNsBqusHirf0x7rXPc9cZqamp9X1u+eVcZg7r5h/p1at+O703sy5Uhfmu9Pl3iOHtED56dv5WCQFdJsPP5HCuydvPbmV9y87+WM6BrR/5z6yTOGN79qINfJNyEVct/ybbddI+P4YFpw/nxa6t4ct4W7jhtEODvEsrYWcpl9cZu//7CkV6V2qruOXsIn2zYyd8/3cwfLgrO91xZU8vM1bnMTS9g8dYiivZVER0VwbmjevI/3xlJvLpmRI5I2IS/c44lW4uY2L8LF4xJ4fWl27mhvnIAAAtRSURBVHlvTd6B8M/dW8G+qloGdjv0ST6hqH/Xjlx1XB9eXbqd6yenMeAwJzq1pqoaHy8vzuLpz7eSX1JBz4T2nDy4KycOSmbq8O7qjxc5SmET/plF5ewqreT4/v7x7BP6JvGPORnsr6olNjqSTTv/e7A3HP349EG8vXIHj3yYzpPXjPe6HABWZxdz31tr2ZhfyvH9OvPIZaM5cWBySJ5zIdLawib8lwQu1FZ3gHN4SgI+BxvzSzimTxKrtxcTYTA8hEf2HErXTjH84OQB/OWTTazI2s14Dy9kV15Vw6Mfb+KFhdvo2imGp68Zz5kjenhWj0goCpsDvku27SY5cFITwMhe/pD/MrcEgBVZexjSIz6suxFumtKPrp1i+OOsjTjncM5R3eCgeEtbum03Z/71c55bsI2rju/DJ3efrOAXaQFh0fKv6+8/vl/nA10GvRJjSYxrx5e5e6mp9bFq+x4uHpfqcaXeiouO4u6pg7n/7XX8/v2vWLJ1NznF+3n/RyfSu3Nci2//3dU5/Pw/a0lJbM/0H0ziuH6heckJkWAQFi3/HXv2k7u34kB/P/hvbzgyJYH1OSVszC9lX1UtE9JC90zeprpsfCoDu3XkhYWZ1PoctT7HPW+uxedzLbbNssoa/t9H6dz5+mqO6ZPIu7efqOAXaWFh0fLfsWc/AAMbjGIZkRLPCwszWRw4HjCuj8I/KjKCF284lp0llYzrk8gby7K57+11vLwki2sDdwJrLs45nluwjcfnZLCnvJpLxqXy0MUjQ/qkOpFgERbhX1pRDUB87Nf780f0SqCq1scby7LpHh9DalKsF+UFndSkOFKT/N08lx/bm1nr8/njrI2cMaw7KYlHv492lVawMquYM4b5z8L9w/sbeH7hNk4e3JW7pw5mTO/E5noLInIYYRL+NQB0av/1tzsyMLJn864ypo3qqSGEjTAz/ufCkZz0yBxmrMrh9lMHHtV6vsgo5Mevr6KwrIr+yR0YnZrAO6tzuWFyGr8+b7j2vUgrC4s+/7qWf8ORPGldOtAh2t/FEIp36moufbrEMb5vEu+tyT2i19XU+vgio5BfzFjH955bQmJcNH+6eBRRkcY7q3O5ZmJfBb+IR8K65R8RYQxPiQ/czEThfyjTRvXk9+9/RcausiadBb15Zyk3vbScrKJy2reL4LsTevOr84bTISaKyyb0ZkNeCcN7xiv4RTwSHi3/yhrat4to9Hru4/omkRDbjmE9w/PkrqaaNronZv6rfx7OwoxCLn7iC8qrann8qnGs/NVU/nTJaDrE+L98IyOMkb0S2vT9A0TauvAI/4rqg5689ZPTBzP7zimHvNGHQPf49hyb1pn3136962drQRlz0ncdmF6wuZDrnl9KSkIsM247gWmjexIXHRY/MEXalLBIvJKKGjrFNB5AsdGR32oESzg5f3RPNu8qY92OvTjnmL48m3P/Pp8bXljGH2dvID2/lB++vIIBXTsy/dZJB0YMiUjwCYsmWWlFzTf6++XInT2yJ394fwPn/2MByR2jKSyrYlL/LqQlx/HUvK28sCCT+Nh2PHf9BBJiw/cyGSJtQVgk4qG6faTpunaK4Z3bJzN/cwHp+aUM7dmJ75/YnwiD/skdefGLTP559Ti1+EXagDAJ/xp6xLf3uoyQMDwlvtErn958Un9uPqm/BxWJyNEIiz5/f8s/LL7nRESaJEzCv0bdPiIi9YR8+NfU+iivqlXLX0SknpAP/7LKurN71fIXEakT8uF/sEs7iIiEs5AP/5K6yzkr/EVEDgj58P9vy1/dPiIidcIo/NXyFxGpEwbh3/i1/EVEwlkYhL9a/iIiDYVB+Ne1/BX+IiJ1Qj/8K2uIjoogJirS61JERIJG6Id/RY2GeYqINBAW4a+DvSIiXxcG4a8reoqINBQG4a+7eImINBQG4V9Npxh1+4iI1BcG4a+Wv4hIQ2ES/mr5i4jUF9LhX+tzlFWq5S8i0lBIh/9/b+Si8BcRqS+kw7/0wLX81e0jIlJfiIe/Wv4iIo0Jk/BXy19EpL4QD39d0VNEpDEhHv7q9hERaUyIh7/u4iUi0piQDv9eSbGcPaKHWv4iIg2EdCqeNrQ7pw3t7nUZIiJBJ6Rb/iIi0jiFv4hIGFL4i4iEIYW/iEgYUviLiIQhhb+ISBhS+IuIhCGFv4hIGDLnnNc1HJaZFQBZR/nyZKCwGctpDW2t5rZWL7S9mlVvy2trNTel3r7Oua6NLWgT4f9tmNly59wEr+s4Em2t5rZWL7S9mlVvy2trNX/betXtIyIShhT+IiJhKBzC/2mvCzgKba3mtlYvtL2aVW/La2s1f6t6Q77PX0REvikcWv4iItKAwl9EJAyFdPib2dlmlm5mGWZ2n9f1NGRmvc1sjpl9ZWZfmtmdgfmdzewTM9sc+G+S17XWZ2aRZrbKzN4PTPczsyWB/fyGmUV7XWN9ZpZoZm+a2UYz22Bmk4J5H5vZXYF/D+vN7DUzax9s+9jMnjezXWa2vt68Rvep+f09UPtaMxsXRDU/Evh3sdbMZphZYr1l9wdqTjezs4Kh3nrLfmpmzsySA9NHvI9DNvzNLBJ4HDgHGA5caWbDva3qG2qAnzrnhgMTgdsDNd4HfOqcGwR8GpgOJncCG+pNPwz81Tk3ENgDfN+Tqg7uMeBD59xQYAz+2oNyH5tZL+DHwATn3EggEriC4NvHLwJnN5h3sH16DjAo8HcL8EQr1djQi3yz5k+Akc650cAm4H6AwOfwCmBE4DX/DGRKa3qRb9aLmfUGzgS215t95PvYOReSf8Ak4KN60/cD93td12FqfheYCqQDPQPzegLpXtdWr8ZU/B/s04D3AcN/lmFUY/vd6z8gAdhGYHBDvflBuY+BXkA20Bn/bVbfB84Kxn0MpAHrD7dPgaeAKxt7ntc1N1j2HeCVwOOv5QXwETApGOoF3sTfiMkEko92H4dsy5//fojq7AjMC0pmlgYcAywBujvn8gKL8oFguhHx34B7AF9gugtQ7JyrCUwH237uBxQALwS6qp41sw4E6T52zuUA/w9/qy4P2AusILj3cZ2D7dO28lm8EZgdeByUNZvZhUCOc25Ng0VHXG8oh3+bYWYdgbeAnzjnSuovc/6v8aAYj2tm5wG7nHMrvK7lCEQB44AnnHPHAPto0MUTZPs4CbgQ/5dWCtCBRn76B7tg2qdNYWYP4O+GfcXrWg7GzOKAXwC/bo71hXL45wC9602nBuYFFTNrhz/4X3HOvR2YvdPMegaW9wR2eVVfA5OBC8wsE3gdf9fPY0CimUUFnhNs+3kHsMM5tyQw/Sb+L4Ng3cdnANuccwXOuWrgbfz7PZj3cZ2D7dOg/iya2fXAecDVgS8tCM6aB+BvFKwJfAZTgZVm1oOjqDeUw38ZMCgwSiIa/8GbmR7X9DVmZsBzwAbn3F/qLZoJXBd4fB3+YwGec87d75xLdc6l4d+fnznnrgbmAJcGnhY09QI45/KBbDMbEph1OvAVQbqP8Xf3TDSzuMC/j7p6g3Yf13OwfToTuDYwImUisLde95CnzOxs/N2YFzjnyustmglcYWYxZtYP/4HUpV7UWMc5t8451805lxb4DO4AxgX+jR/5PvbioEsrHiw5F/8R/C3AA17X00h9J+L/abwWWB34Oxd/P/qnwGbg/4DOXtfaSO2nAO8HHvfH/8HIAP4DxHhdX4NaxwLLA/v5HSApmPcx8DtgI7Ae+DcQE2z7GHgN/zGJ6kAIff9g+xT/oIDHA5/DdfhHMgVLzRn4+8rrPn9P1nv+A4Ga04FzgqHeBssz+e8B3yPex7q8g4hIGArlbh8RETkIhb+ISBhS+IuIhCGFv4hIGFL4i4iEIYW/iEgYUviLiISh/w8N6wxQ9TzO4QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.title(f'Total Rewards')\n",
        "plt.yscale('symlog')\n",
        "plt.plot(totalRewards)\n",
        "plt.savefig(\"Total Rewards\",dpi=200)\n",
        "plt.clf()\n",
        "plt.title(f'Average Rewards')\n",
        "plt.yscale('symlog')\n",
        "plt.plot(averageRewards)\n",
        "plt.savefig(\"Average Rewards\",dpi=200)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Ddeuling_DQN_for_Gym_LunarLander.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}